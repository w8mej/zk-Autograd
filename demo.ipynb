{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fde24a58",
   "metadata": {},
   "source": [
    "# zk-Autograd: Provable Training Demo\n",
    "\n",
    "This notebook demonstrates the core features of **zk-Autograd**, a framework for generating Zero-Knowledge (ZK) proofs of training steps for PyTorch models. It showcases:\n",
    "\n",
    "1.  **Setup**: Compiling ZK circuits for the optimizer (Adam).\n",
    "2.  **Training**: Running a provable training loop with a toy model.\n",
    "3.  **Audit Log**: Inspecting the cryptographic audit log and Merkle tree.\n",
    "4.  **Verification**: Verifying the generated ZK proofs.\n",
    "5.  **Advanced Features**: Triton kernel integration and proof chunking.\n",
    "6.  **Decentralized Storage**: Packaging artifacts into a torrent bundle.\n",
    "7.  **Security**: Fuzzing the verifier to ensure robustness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7413b3",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, we install the package in editable mode and verify that the necessary dependencies (EZKL, PyTorch) are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b44b00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e .\n",
    "import torch\n",
    "import ezkl\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"EZKL version: {ezkl.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2593ef",
   "metadata": {},
   "source": [
    "### Circuit Compilation\n",
    "\n",
    "We compile the ZK circuit for the Adam optimizer. This generates the proving key (`pk.key`), verification key (`vk.key`), and the compiled circuit (`compiled.ezkl`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a3f134",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zk-setup-zk --circuit adam --dim 128 --out prover/keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c97f33",
   "metadata": {},
   "source": [
    "## 2. Provable Training Loop\n",
    "\n",
    "We now run a short training session using `zk_autograd.trainer`. This will:\n",
    "- Train a `TinyMLP` model on synthetic data.\n",
    "- Capture execution traces (witnesses) for each optimization step.\n",
    "- Generate ZK proofs (if a prover is available/configured).\n",
    "- Log everything to a tamper-evident audit log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6b7c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zk_autograd.trainer import run_demo_train\n",
    "\n",
    "# Run 5 steps of training\n",
    "run_dir = run_demo_train(steps=5, prover_url=\"http://127.0.0.1:8000\")\n",
    "print(f\"Training complete. Artifacts stored in: {run_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d43699",
   "metadata": {},
   "source": [
    "## 3. Inspecting the Audit Log\n",
    "\n",
    "The training run produces a `steps.jsonl` file containing the metadata and proof hashes for each step. These hashes are aggregated into a Merkle tree to ensure integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b055c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "log_path = os.path.join(run_dir, \"steps.jsonl\")\n",
    "with open(log_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        step = json.loads(line)\n",
    "        print(f\"Step {step['step_idx']}: Loss={step['loss']:.4f}, Proof Hash={step['proof_hash'][:16]}...\")\n",
    "\n",
    "merkle_root_path = os.path.join(run_dir, \"merkle_root.txt\")\n",
    "if os.path.exists(merkle_root_path):\n",
    "    print(f\"\\nMerkle Root: {open(merkle_root_path).read().strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82aef33",
   "metadata": {},
   "source": [
    "## 4. Verification\n",
    "\n",
    "We can randomly sample steps from the run and verify their ZK proofs against the committed public inputs (weights, gradients, hyperparameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf48f73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from verifier.verify_steps import verify_random_steps\n",
    "\n",
    "# Verify 3 random steps from the run\n",
    "verify_random_steps(run_dir, k=3, key_dir=\"prover/keys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6877d558",
   "metadata": {},
   "source": [
    "## 5. Advanced Features\n",
    "\n",
    "### Triton Kernels\n",
    "zk-Autograd supports custom Triton kernels for high-performance fused Adam updates on GPU. This ensures that the execution trace matches the ZK circuit logic exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb3cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zk_autograd.triton_kernels import available\n",
    "\n",
    "if available():\n",
    "    print(\"Triton/CUDA is available! Using fused kernels for training.\")\n",
    "else:\n",
    "    print(\"Triton/CUDA not detected. Falling back to pure PyTorch implementation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0d2510",
   "metadata": {},
   "source": [
    "### Proof Chunking\n",
    "For large models, generating a single monolithic proof is infeasible. We support splitting the model update into smaller chunks, proving them in parallel, and aggregating the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2fa023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zk_autograd.splitting import plan_split\n",
    "\n",
    "# Example: Plan a split of the Adam step into 4 chunks\n",
    "plan = plan_split(\"prover/keys/adam_step.onnx\", chunks=4, out_dir=\"prover/keys/chunks\")\n",
    "print(\"Split Plan:\", json.dumps(plan, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de6fac6",
   "metadata": {},
   "source": [
    "## 6. Decentralized Storage (Torrents)\n",
    "\n",
    "To ensure availability and censorship resistance, we package the training artifacts (proofs, logs, models) into a torrent bundle. This allows anyone to seed and download the verifiable training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070300df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zk_autograd.torrents import create_toy_torrent_bundle\n",
    "\n",
    "# Create a torrent bundle for the run directory\n",
    "manifest = create_toy_torrent_bundle(run_dir, out_dir=\"public/torrents\")\n",
    "print(f\"Torrent created: {manifest['magnet']}\")\n",
    "print(\"Files included:\", [f['path'] for f in manifest['files']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supply-chain-markdown",
   "metadata": {},
   "source": [
    "### Supply Chain Security\n",
    "\n",
    "In a production environment, the Docker images used for the prover and trainer are signed using **Sigstore (Cosign)**, and an SBOM is generated to ensure supply chain integrity. This prevents tampering with the software artifacts before they are deployed to the TEE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzing-markdown",
   "metadata": {},
   "source": [
    "## 7. Security: Verifier Fuzzing\n",
    "\n",
    "Security is paramount. We use property-based fuzzing (Hypothesis) to ensure the verifier is robust against malformed inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzing-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest tests/test_fuzz_verifier.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
